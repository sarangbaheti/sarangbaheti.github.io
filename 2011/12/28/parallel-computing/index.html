<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Parallel Computing - a few notes</title><meta name=description content="a few parallel computing terms"><meta name=author content="Sarang Baheti"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"a few notes","url":"https:\/\/www.variadic.xyz\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"https:\/\/www.variadic.xyz\/"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/www.variadic.xyz\/","name":"home"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/www.variadic.xyz\/2011\/12\/28\/parallel-computing\/","name":"Parallel computing"}}]}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"name":"Sarang Baheti"},"headline":"Parallel Computing","description":"a few parallel computing terms","inLanguage":"en","wordCount":1326,"datePublished":"2011-12-28T00:00:00","dateModified":"2011-12-28T00:00:00","image":"https:\/\/www.variadic.xyz\/","keywords":["parallel"],"mainEntityOfPage":"https:\/\/www.variadic.xyz\/2011\/12\/28\/parallel-computing\/","publisher":{"@type":"Organization","name":"https:\/\/www.variadic.xyz\/","logo":{"@type":"ImageObject","url":"https:\/\/www.variadic.xyz\/","height":60,"width":60}}}</script><meta property="og:title" content="Parallel Computing"><meta property="og:description" content="a few parallel computing terms"><meta property="og:url" content="https://www.variadic.xyz/2011/12/28/parallel-computing/"><meta property="og:type" content="website"><meta property="og:site_name" content="a few notes"><meta name=twitter:title content="Parallel Computing"><meta name=twitter:description content="a few parallel computing terms"><meta name=twitter:card content="summary"><meta name=generator content="Hugo 0.73.0"><link rel=alternate href=https://www.variadic.xyz/index.xml type=application/rss+xml title="a few notes"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://www.variadic.xyz/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://www.variadic.xyz/css/syntax.css><link rel=stylesheet href=https://www.variadic.xyz/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-27935503-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><nav class="navbar navbar-default navbar-fixed-top navbar-custom"><div class=container-fluid><div class=navbar-header><button type=button class=navbar-toggle data-toggle=collapse data-target=#main-navbar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=https://www.variadic.xyz/>a few notes</a></div><div class="collapse navbar-collapse" id=main-navbar><ul class="nav navbar-nav navbar-right"><li><a title=Notes href=/>Notes</a></li><li><a title=About href=/about/>About</a></li><li><a title=Archive href=/archive/>Archive</a></li><li><a title=Books href=/books/>Books</a></li><li><a title=Disclaimer href=/disclaimer/>Disclaimer</a></li></ul></div></div></nav><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1>Parallel Computing</h1><span class=post-meta>Posted on December 28, 2011</span></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><div class=blog-tags><a href=https://www.variadic.xyz//tags/parallel/>parallel</a>&nbsp;</div><p style=text-align:justify>Parallel computing is a form of computation in which many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved concurrently (&#8220;in parallel&#8221;). There are several different forms of parallel computing: bit-level, instruction level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multicore processors.</p><p>Parallelism can be broadly classified in three types:</p><ol><li>Instruction-level</li><li>Data Parallel</li><li>Task Parallel</li></ol><h2 id=instruction-level-parallelism-ilp>Instruction-Level Parallelism (ILP)</h2><p style=text-align:justify>Instruction-level Parallelism is a family of processor and compiler design techniques that speeds up execution by causing individual machine operations to execute in parallel. Although ILP has appeared in the highest performance uniprocessors for the past 30 years, the 1980s saw it become a much more significant force in computer design.</p><p style=text-align:justify>Three of the most common techniques for ILP are:</p><h3 id=superscalar-execution>Superscalar Execution</h3><p style=text-align:justify>A superscalar CPU architecture implements a form of parallelism called instruction level parallelism within a single processor. It therefore allows faster CPU throughput than would otherwise be possible at a given clock rate. A superscalar processor executes more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor. Each functional unit is not a separate CPU core but an execution resource within a single CPU such as an arithmetic logic unit, a bit shifter, or a multiplier.</p><blockquote><p style=text-align:justify>Please refer <a href=http://nostarch.com/download/insidemachine_ch4.pdf>Chapter 4</a> from <a href=http://www.amazon.com/Inside-Machine-Introduction-Microprocessors-Architecture/dp/1593271042>Inside The Machine</a> by Jon Stokes for excellent coverage of superscalar architecture.</p></blockquote><p style=text-align:justify>The major advantage of the Superscalar/Out-of-Order Execution lies with the software developer, as the hardware extracts the parallelism from programmer&#8217;s code automatically without any  efforts from programmer. On the other hand it requires a substantial area on the CPU die to maintain dependence information and queues of instructions.</p><h3 id=vliw-very-large-instruction-word>VLIW (very large instruction word)</h3><p style=text-align:justify>Very long instruction word or VLIW refers to a CPU architecture designed to take advantage of instruction level parallelism (ILP). A processor that executes every instruction one after the other (i.e. a non-pipelined scalar architecture) may use processor resources inefficiently, potentially leading to poor performance. The performance can be improved by executing different sub-steps of sequential instructions simultaneously (this is <em>pipelining</em>), or even executing multiple instructions entirely simultaneously as in superscalar architectures.</p><blockquote><p style=text-align:justify>Refer these two articles from Jon Stokes of ArsTechnica for excellent coverage of Pipelining in Microprocessors:</p><ol><li><a href=http://arstechnica.com/paedia/c/cpu/part-1/cpu1-1.html>Understanding the Microprocessor Part-1</a></li><li><a href=http://arstechnica.com/paedia/c/cpu/part-2/cpu2-1.html>Understanding the Microprocessor Part-2</a></li></ol></blockquote><p style=text-align:justify>VLIW primarily depends on Compiler to generate interleaved and efficient code for parallel execution.</p><h3 id=simd-vector>SIMD/Vector</h3><p style=text-align:justify>A vector processor, or array processor, is a central processing unit (CPU) that implements an instruction set containing instructions that operate on one-dimensional arrays of data called <em>vectors</em>. This is in contrast to a scalar processor, whose instructions operate on single data items.</p><p style=text-align:justify>Vector processing techniques have been added to almost all modern CPU designs, although they are typically referred to as SIMD (Single Instruction Multiple Data). In these implementations, the vector unit runs beside the main scalar CPU, and is fed data from programs that know it is there.</p><p style=text-align:justify>SIMD tackles extraction of parallelism in different way as compared to Superscalar and VLIW ILP, while both of these extract independent ILP from unrelated instructions, SIMD allows the hardware instructions to target data parallel executions. The bigger benefit comes from fact that SIMD instructions are generated by Compiler/Developer rather than Processor figuring it out during execution.</p><blockquote><p style=text-align:justify>SIMD code is also generated automatically by various compilers, also known as loop-unrolling or vectorization, as part of optimization. See <a title="auto vectorization" href=/2011/10/02/auto-vectorization/>here</a> for some details on how VS 2011 generates <a href=http://en.wikipedia.org/wiki/Advanced_Vector_Extensions>AVX</a> (Advanced Vector Extensions) instructions for computation acceleration.</p></blockquote><p style=text-align:justify>SIMD is very commonly used in media and scientific applications for computation acceleration, as the arithmetic density is quite high as compared to control instructions.</p><h2 id=data-parallelism>Data Parallelism</h2><p style=text-align:justify>Data parallelism (also known as loop-level parallelism) is a form of parallelization of computing across multiple processors in parallel computing environments. Data parallelism focuses on distributing the data across different parallel computing nodes.</p><p style=text-align:justify>Data parallelism is abstraction for doing certain operation on different data. It encompasses SIMD and SPMD (Single Program Multiple Data). It is heavily used in graphics rendering or solving large linear systems as these problems can be very easily expressed in data parallel model.</p><blockquote><p style=text-align:justify>The data-parallel programming model is both higher level and more restrictive than the task model. It is <em>higher level </em>in sense that the programmer is not required to specify communication structures explicitly: these are derived by a compiler from the domain decomposition specified by the programmer. It is <em>more restrictive </em>because not all algorithms can be specified in data-parallel terms. For these reasons, data parallelism, although important, is not a universal parallel programming paradigm.</p><p style=text-align:justify><a href=http://www.mcs.anl.gov/~itf/dbpp/text/node83.html#SECTION03413000000000000000>Source</a></p></blockquote><h2 id=task-parallelism>Task Parallelism</h2><p style=text-align:justify>Task parallelism (also known as function parallelism and control parallelism) is a form of parallelization of computer code across multiple processors in parallel computing environments. Task parallelism focuses on distributing execution processes (threads) across different parallel computing nodes. It contrasts to data parallelism as another form of parallelism.</p><blockquote><p style=text-align:justify>The task-parallel programming model is less restrictive than the data-parallel model. It is lower level in sense that the programmer is required to specify communication structures explicitly, in case of task based libraries, like <a href=http://threadingbuildingblocks.org/>TBB</a> or <a href=http://msdn.microsoft.com/en-us/library/dd492418.aspx>PPL</a>, communication and control logic is mostly taken care by concurrency runtime. It can model almost all kinds of parallel programming models.</p></blockquote><p style=text-align:justify>In a multiprocessor system, task parallelism is achieved when each processor executes a different thread (or process) on the same or different data. The threads may execute the same or different code. In the general case, different execution threads communicate with one another as they work. Communication takes place usually to pass data from one thread to the next as part of a workflow.</p><h3 style=text-align:justify>Summary</h3><p style=text-align:justify>In this post I have tried to categorize Parallel Computing in terms of models used to represent the work being done. ILP falls mostly in the domain of hardware and compilers, developers can also write SIMD code but it is better to leave it to Compilers. ILP defines very fine grained level of parallelism and is mostly an implementation technique. While Data and Task parallelism are more of architectural models, they define granularity of parallel work, and at times entire workflow, for your application. Parallelism defined by Data & Task Parallel models is at much coarser level as compared to ILP.</p><p style=text-align:justify>Clearly there is no one model that is universal, in terms of both expressing parallelism and extracting superior performance. Task Parallelism is suitable for expressing concurrency, where workdone is of different nature and/or operates on different data, as it give more control over the entire workflow. Data Parallelism is an excellent choice for doing tons of calculations on a single set of data, e.g., GPU programming, where arithmetic instruction density is very high as compared to control instructions. ILP is great for extracting out the last bit of performance from executable code. However in most of the real life applications you will get the better results by incorporating synergistic combination of these three models.</p><h6 style=font-size:1em>Related articles</h6><ul><li><a href=http://www.gotw.ca/publications/concurrency-ddj.htm>The Free Lunch is over</a> (gotw.ca)</li><li><a href=/2011/12/openacc-programming-standard-for-parallel-computing-2/>&#8216;OpenACC&#8217; Programming Standard for Parallel Computing</a></li><li><a href=http://www.hpl.hp.com/techreports/92/HPL-92-132.pdf>Instruction Level Parallel Processing: History, Overview and Perspective</a> (hpl.hp.com)</li><li><a href=https://computing.llnl.gov/tutorials/parallel_comp/>Introduction to Parallel Computing</a> (computing.llnl.gov)</li><li><a href=http://www.mcs.anl.gov/~itf/dbpp/>Designing and Building Parallel Programs (dbpp)</a> (mcs.anl.gov)</li><li><a href=http://arstechnica.com/paedia/c/cpu/part-1/cpu1-1.html>Understanding the Microprocessor Part-1</a> (arstechnica.com)</li><li><a href=http://arstechnica.com/paedia/c/cpu/part-2/cpu2-1.html>Understanding the Microprocessor Part-2</a> (arstechnica.com)</li><li><a href=http://nostarch.com/download/insidemachine_ch4.pdf>SuperScalar Execution &#8211; Inside The Machine</a> (nostarch.com)</li><li><a href=/2011/12/concurrency-and-parallelism/>concurrency and parallelism</a></li><li><a title="auto vectorization" href=/2011/10/02/auto-vectorization/>auto vectorization</a> </li><li><a href=http://www.classic.nxp.com/acrobat_download2/other/vliw-wp.pdf>Introduction to VLIW Computer Architecture</a> (classic.nxp.com)</li><li><a title="The Pentium: An Architectural History of the World’s Most Famous Desktop Processor" href=/2011/05/30/the-pentium-an-architectural-history-of-the-worlds-most-famous-desktop-processor/>The Pentium: An Architectural History of the World’s Most Famous Desktop Processor</a> </li><li><a href=http://en.wikipedia.org/wiki/Advanced_Vector_Extensions>Advanced Vector Instructions</a> (wikipedia.org)</li><li><a href=http://msdn.microsoft.com/en-us/library/gg675934.aspx>Parallel Programming with VC++</a> (msdn.microsoft.com)</li><li><a title="C++AMP: Accelerated Massive Parallelism" href=/2011/08/15/camp-accelerated-massive-parallelism/>C++AMP: Accelerated Massive Parallelism</a></li><li><a href=http://insidehpc.com/2011/09/29/intels-opencl-autovectorization-boosts-performance-without-user-intervention/>Intel&#8217;s OpenCL Autovectorization Boosts Performance without User Intervention</a> (insidehpc.com)</li><li><a href=http://www.khronos.org/opencl/>OpenCL</a> (khronos.org)</li><li><a href=http://developer.amd.com/zones/openclzone/pages/default.aspx>OpenCL Zone</a> (amd.com)</li><li><a href="https://graphics.stanford.edu/wikis/cs248-11-winter/CS_248%3A_Interactive_Computer_Graphics?action=AttachFile&do=get&target=GraphicsHardware.pdf">How GPUs work &#8211; Kayvon Fatahalian</a> (graphics.stanford.edu)</li><li><a title="Multithreading in C++11 – part 1" href=/2011/09/16/multithreading-in-c11-part-1/>Multithreading in C++11 – part 1</a> </li><li><a title="Multithreading in C++11 – part 2" href=/2011/09/18/multithreading-in-c11-part-2/>Multithreading in C++11 – part 2</a></li></ul><br><div class=blog-tags><a href=https://www.variadic.xyz//tags/parallel/>parallel</a>&nbsp;</div><hr></article><ul class="pager blog-pager"><li class=previous><a href=https://www.variadic.xyz/2011/12/23/concurrency-and-parallelism/ data-toggle=tooltip data-placement=top title="concurrency and parallelism">&larr; Previous Post</a></li><li class=next><a href=https://www.variadic.xyz/2011/12/30/a-guide-to-undefined-behavior-in-c-and-c/ data-toggle=tooltip data-placement=top title="A Guide to Undefined Behavior in C and C++">Next Post &rarr;</a></li></ul></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"></ul><p class="credits copyright text-muted">Sarang Baheti
&nbsp;&bull;&nbsp;&copy;
2020
&nbsp;&bull;&nbsp;
<a href=https://www.variadic.xyz/>a few notes</a></p><p class="credits theme-by text-muted"><a href=https://gohugo.io>Hugo v0.73.0</a> powered &nbsp;&bull;&nbsp; Theme <a href=https://github.com/sarangbaheti/beautifulhugo>beautifulhugo</a> forked from <a href=https://github.com/halogenica/beautifulhugo>halogenica/Beautiful Hugo</a></p></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe crossorigin=anonymous></script><script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script src=https://www.variadic.xyz/js/main.js></script><script>renderMathInElement(document.body);</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://www.variadic.xyz/js/load-photoswipe.js></script></body></html>